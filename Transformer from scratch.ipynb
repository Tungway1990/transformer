{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTPHvUz4LLiD"
   },
   "source": [
    "This is a notebook to build vanillia transformer form scratch.\n",
    "\n",
    "Orinigal paper: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "Make reference to: https://www.tensorflow.org/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2567,
     "status": "ok",
     "timestamp": 1608356533717,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "CeOXhTk9LLiJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOZpKKAjLLiK"
   },
   "source": [
    "### Positional ecoding\n",
    "\n",
    "Notes to positional encoding:\n",
    "\n",
    "1. Input \n",
    "    \n",
    "    L: Length of sequence (number of words in a sentence)\n",
    "    \n",
    "    d: number of neuron in later encoder/decoder layer\n",
    "\n",
    "3. Output is a 3-D array (1, batch size, d_model) which to be added with embedding layer\n",
    "\n",
    "   Formula:\n",
    "$$\\large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1608356533718,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "4bcHglp-LLiK",
    "outputId": "dc380862-4930-49ea-8a1a-b21d28af916f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "image/png": {
       "height": 300,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Graphical visualization\n",
    "'''\n",
    "Image(filename = \"positional encoding.png\", width = 400, height = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2554,
     "status": "ok",
     "timestamp": 1608356533719,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "Kk1V3dt7LLiM"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def positional_encoding(L,d): \n",
    "  X=(np.zeros((d,L))+np.arange(L)).T\n",
    "  X=X/(10000**(2*(np.arange(d)//2)/d))\n",
    "  X[:,0::2]=np.sin(X[:,0::2])\n",
    "  X[:,1::2]=np.cos(X[:,1::2])\n",
    "  X=X[np.newaxis,:]\n",
    "  return tf.cast(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F13Fz62WLLiM"
   },
   "source": [
    "## Scaled dot product attaention\n",
    "\n",
    "Input shape\n",
    "q,k,v dimension 4D array (batch_size, number of heads, Length of sequence, depth)\n",
    "\n",
    "Output shape\n",
    "(batch size, number of heads, depth)\n",
    "\n",
    "Encoder:\n",
    "\n",
    "1. q,k and v are all come from X, the input vector\n",
    "\n",
    "2. Masking is not needed\n",
    "\n",
    "Decoder:\n",
    "\n",
    "1. q is the decoder output, while k and v are the same encoder output\n",
    "\n",
    "2. Masking is needed\n",
    "\n",
    "$$\\small{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2550,
     "status": "ok",
     "timestamp": 1608356533720,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "cYQSrxkeLLiM"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask):\n",
    "  QK=tf.matmul(q,k,transpose_b=True)/tf.math.sqrt(tf.cast(tf.shape(k)[-1],tf.float32))\n",
    "  if mask is not None:\n",
    "    QK+= mask * -1e9\n",
    "  weight=tf.nn.softmax(QK,axis=-1)\n",
    "  otuput=tf.matmul(weight,v)\n",
    "  return otuput, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncmBjrzMLLiN"
   },
   "source": [
    "### Multihead attention\n",
    "\n",
    "input shape: batch size, sequence length, model depth\n",
    "\n",
    "\n",
    "output shape: batch size, head size, sequence length, number of neuron in each head \n",
    "\n",
    "(model depth = head size x number of neuron in each head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2544,
     "status": "ok",
     "timestamp": 1608356533720,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "OLqReHTVLLiN"
   },
   "outputs": [],
   "source": [
    "class multihead_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,name='Multihead'):\n",
    "        super(multihead_attention,self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.d_model=d_model\n",
    "        self.depth=self.d_model//self.num_heads\n",
    "\n",
    "        self.q_layer=tf.keras.layers.Dense(self.d_model)\n",
    "        self.k_layer=tf.keras.layers.Dense(self.d_model)\n",
    "        self.v_layer=tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.dense=tf.keras.layers.Dense(self.d_model)\n",
    "    def call(self,q,k,v,mask):\n",
    "        batch_size=tf.shape(q)[0]\n",
    "\n",
    "        q=self.q_layer(q)\n",
    "        k=self.k_layer(k)\n",
    "        v=self.v_layer(v)\n",
    "\n",
    "        #(batch_size, num_heads, seq_len_, depth)\n",
    "        q=tf.reshape(q,(batch_size,-1,self.num_heads,self.depth))\n",
    "        q=tf.transpose(q,perm=[0,2,1,3]) \n",
    "        k=tf.reshape(k,(batch_size,-1,self.num_heads,self.depth))\n",
    "        k=tf.transpose(k,perm=[0,2,1,3])\n",
    "        v=tf.reshape(v,(batch_size,-1,self.num_heads,self.depth))\n",
    "        v=tf.transpose(v,perm=[0,2,1,3])\n",
    "        \n",
    "        scaled_attention, attention_weights=scaled_dot_product_attention(q,k,v,mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) #(B,L,H,D)\n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model)) \n",
    "        concat_attention=self.dense(concat_attention)\n",
    "\n",
    "        return concat_attention,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y52502r6LLiN"
   },
   "source": [
    "### Enocder Block\n",
    "The whole block of encoder layer consist of:\n",
    "\n",
    "1. multihead attention\n",
    "2. layer normalization (residual connection)\n",
    "3.feed forward network\n",
    "    3.1 Dense layer sized 2048 with Relu activation\n",
    "    3.2 Dense layer sized 512\n",
    "4.layer normalization (residual connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2540,
     "status": "ok",
     "timestamp": 1608356533721,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "nv74Y6NULLiO"
   },
   "outputs": [],
   "source": [
    "class encoder_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_heads,d_model,ffn,drop_rate=0.1):\n",
    "        super(encoder_layer,self).__init__()\n",
    "        self.drop_rate=drop_rate\n",
    "        self.ffn=ffn\n",
    "        self.num_heads=num_heads\n",
    "        self.d_model=d_model\n",
    "        \n",
    "        self.mha=multihead_attention(self.d_model,self.num_heads)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.Drop1=tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.Drop2=tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.Dense1=tf.keras.layers.Dense(self.ffn, activation='relu')\n",
    "        self.Dense2=tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self,X,training,mask):\n",
    "        X_shortcut=X #first residual\n",
    "        X,_=self.mha(X,X,X,mask)\n",
    "\n",
    "        X=self.Drop1(X,training=training)\n",
    "        X=self.layernorm1(X+X_shortcut)\n",
    "        \n",
    "        X_shortcut=X #second residual\n",
    "        '''\n",
    "        Feed forward network\n",
    "        '''\n",
    "        X=self.Dense1(X)\n",
    "        X=self.Dense2(X)\n",
    "        X=self.Drop2(X,training=training)\n",
    "        X=self.layernorm2(X+X_shortcut)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyenGCIXLLiP"
   },
   "source": [
    "## Notes to Embedding layer\n",
    "https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "\n",
    "Input shape: (batch size, input vocab size), not batch input length\n",
    "\n",
    "\n",
    "Output shape: (batch size, input vocab size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2536,
     "status": "ok",
     "timestamp": 1608356533721,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "-1uFOG-RLLiP"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_vocab_size,\n",
    "                 num_heads, ffn, num_layers=6, drop_rate=0.1):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.input_vocab_size=input_vocab_size\n",
    "        self.num_heads=num_heads\n",
    "        self.drop_rate=drop_rate\n",
    "        self.ffn=ffn\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.embedding=tf.keras.layers.Embedding(self.input_vocab_size,self.d_model)\n",
    "        self.position=positional_encoding(self.input_vocab_size, self.d_model)\n",
    "        self.Dropout=tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.encoder_layers=[encoder_layer(self.num_heads,self.d_model,self.ffn,self.drop_rate) \n",
    "                            for _ in range(self.num_layers)]\n",
    "        \n",
    "    def call(self,X_input,training,mask):\n",
    "        input_length=tf.shape(X_input)[1]\n",
    "\n",
    "        X=self.embedding(X_input)\n",
    "        X*=tf.math.sqrt(tf.cast(self.d_model,dtype=tf.float32)) #See paper 3.4\n",
    "        X_pos=self.position[:,:input_length,:]     \n",
    "        X+=X_pos\n",
    "        X=self.Dropout(X,training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            X=self.encoder_layers[i](X,training,mask)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzOe2gd_LLiQ"
   },
   "source": [
    "### Decoder Block\n",
    "\n",
    "The whole block of decoder layer consist of:\n",
    "\n",
    "1. masked multihead attention\n",
    "2. layer normalization (residual connection)\n",
    "3. multihead attention\n",
    "4. layer normalization (residual connection)\n",
    "5.feed forward network\n",
    "\n",
    "    5.1 Dense layer sized 2048 with Relu activation\n",
    "    \n",
    "    5.2 Dense layer sized 512\n",
    "6.layer normalization (residual connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2531,
     "status": "ok",
     "timestamp": 1608356533722,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "AdTD6z-8LLiQ"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2528,
     "status": "ok",
     "timestamp": 1608356533722,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "g8gtvq25LLiQ"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2524,
     "status": "ok",
     "timestamp": 1608356533722,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "dl0I8qotLLiR"
   },
   "outputs": [],
   "source": [
    "class decoder_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model,ffn, num_heads, drop_rate=0.1):\n",
    "        super(decoder_layer,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.ffn=ffn\n",
    "        self.drop_rate=drop_rate\n",
    "        self.num_heads=num_heads\n",
    "        \n",
    "        self.mha_mask=multihead_attention(self.d_model,self.num_heads)\n",
    "        self.mha=multihead_attention(self.d_model,self.num_heads)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.Dropout1 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.Dropout2 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.Dropout3 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.Dense1=tf.keras.layers.Dense(self.ffn, activation='relu')\n",
    "        self.Dense2=tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self,Y,encoder_output,training,look_ahead_mask,padding_mask):\n",
    "        Y_shortcut=Y\n",
    "        Y,attn_weights_block1=self.mha_mask(Y,Y,Y,look_ahead_mask) \n",
    "        Y=self.Dropout1(Y,training=training)\n",
    "        Y=self.layernorm1(Y+Y_shortcut)\n",
    "        \n",
    "        Y_shortcut=Y\n",
    "        Y,attn_weights_block2=self.mha(Y,encoder_output,encoder_output,padding_mask)\n",
    "        Y=self.Dropout2(Y,training=training)\n",
    "        Y=self.layernorm2(Y+Y_shortcut)\n",
    "\n",
    "        Y_shortcut=Y\n",
    "        Y=self.Dense1(Y)\n",
    "        Y=self.Dense2(Y)\n",
    "        Y=self.Dropout3(Y,training=training)\n",
    "        Y=self.layernorm3(Y+Y_shortcut)\n",
    "\n",
    "        return Y, attn_weights_block1, attn_weights_block2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2521,
     "status": "ok",
     "timestamp": 1608356533723,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "flYLuj9mLLiR"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,target_vocab_size,num_heads,ffn,num_layers=6,drop_rate=0.1):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.ffn=ffn\n",
    "        self.drop_rate=drop_rate\n",
    "        self.target_vocab_size=target_vocab_size\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "        self.embedding=tf.keras.layers.Embedding(self.target_vocab_size,self.d_model)\n",
    "        self.position=positional_encoding(self.target_vocab_size, self.d_model)\n",
    "        self.Dropout1=tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.decoder_layers=[decoder_layer(self.d_model,self.ffn, self.num_heads, self.drop_rate) \n",
    "                             for _ in range(self.num_layers)]\n",
    "\n",
    "    def call(self,Y_input,encoder_output,training,look_ahead_mask, padding_mask):\n",
    "        target_length = tf.cast(tf.shape(Y_input)[1],dtype=tf.int32)\n",
    "\n",
    "        attention_weights = {}\n",
    "        Y=self.embedding(Y_input)\n",
    "        Y*=tf.math.sqrt(tf.cast(self.d_model,dtype=tf.float32)) #See Section 3.4\n",
    "        Y_pos=self.position[:,:target_length,:]  \n",
    "        \n",
    "        Y+=Y_pos\n",
    "        Y=self.Dropout1(Y,training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            Y,block1,block2=self.decoder_layers[i](Y,encoder_output,training,look_ahead_mask, padding_mask)\n",
    "        \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        return Y,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 1876,
     "status": "ok",
     "timestamp": 1608358968805,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "Pf9IOAXjLLiS"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,d_model, input_vocab_size, target_vocab_size, \n",
    "                  num_heads, ffn, num_layers, drop_rate):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.input_vocab_size=input_vocab_size\n",
    "        self.target_vocab_size=target_vocab_size\n",
    "        self.num_heads=num_heads\n",
    "        self.ffn=ffn\n",
    "        self.drop_rate=drop_rate\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "        self.encod=Encoder(self.d_model, self.input_vocab_size,\n",
    "                           self.num_heads,self.ffn,self.num_layers,self.drop_rate)\n",
    "        self.decod=Decoder(self.d_model, self.target_vocab_size,\n",
    "                           self.num_heads,self.ffn,self.num_layers,self.drop_rate)\n",
    "        self.linear=tf.keras.layers.Dense(self.target_vocab_size, activation='softmax')\n",
    "        \n",
    "    def call(self,X_input, Y_input, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        X=self.encod(X_input,training, enc_padding_mask)\n",
    "        X,attention_weights=self.decod(Y_input, X, training, look_ahead_mask, dec_padding_mask)\n",
    "        output=self.linear(X)\n",
    "\n",
    "        return output,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2515,
     "status": "ok",
     "timestamp": 1608356533724,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "bDvzhWG123FV"
   },
   "outputs": [],
   "source": [
    "#Let's follow the remaining step in the tutorial to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1608356534002,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "fWvfT1KxLLiT"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3929,
     "status": "ok",
     "timestamp": 1608356535146,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "0SZjLyffLLiT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12834,
     "status": "ok",
     "timestamp": 1608356544059,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "an7dIeHXk7m1",
    "outputId": "0b2a3c45-cfbb-45b2-b9ac-57c2704e3771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .', shape=(), dtype=string)\n",
      "tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "'Visualize the data'\n",
    "sample=(list(iter(train_examples))[0])\n",
    "print(sample[0])\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 133553,
     "status": "ok",
     "timestamp": 1608356664785,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "xH5rgrOfLLiT"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 133550,
     "status": "ok",
     "timestamp": 1608356664787,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "VczTcMxmLLiT"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 133546,
     "status": "ok",
     "timestamp": 1608356664788,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "zzQV3v_1LLiU"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144704,
     "status": "ok",
     "timestamp": 1608356675951,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "qu-o6NdoLLiU",
    "outputId": "f99cc1f7-05ec-4561-fed7-5b6448dc5777"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214,  119,    1, ...,    0,    0,    0],\n",
       "        [8214,    6,   70, ...,    0,    0,    0],\n",
       "        [8214,    7, 2533, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,    6, 4051, ...,    0,    0,    0],\n",
       "        [8214, 7847,  628, ...,    0,    0,    0],\n",
       "        [8214,  802,  277, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 36), dtype=int64, numpy=\n",
       " array([[8087,   18,   12, ...,    2, 8088,    0],\n",
       "        [8087,    4,   19, ...,    0,    0,    0],\n",
       "        [8087,    3, 7210, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,    4,   14, ...,    0,    0,    0],\n",
       "        [8087,   12,   31, ...,    0,    0,    0],\n",
       "        [8087,  750, 2879, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(train_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PbWuxuILLiU"
   },
   "source": [
    "Learning rate\n",
    "\n",
    "$$\\small{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 144698,
     "status": "ok",
     "timestamp": 1608356675953,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "5PGyWtThLLiU"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 144694,
     "status": "ok",
     "timestamp": 1608356675954,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "sX457XGoLLiU"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model=512)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 144689,
     "status": "ok",
     "timestamp": 1608356675954,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "PPHs8Az8LLiU"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "  \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 144684,
     "status": "ok",
     "timestamp": 1608356675954,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "4tqW9RVMLLiV"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 144681,
     "status": "ok",
     "timestamp": 1608356675955,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "F--yl5X_LLiV"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1608358988488,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "N7wif5k7LLiV"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(d_model=512, input_vocab_size=tokenizer_pt.vocab_size + 2, \n",
    "                          target_vocab_size=tokenizer_en.vocab_size + 2,\n",
    "                          num_heads=8, ffn=2048, num_layers=2,drop_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 144967,
     "status": "ok",
     "timestamp": 1608356676252,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "tgrLitoULLiV"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1608359036913,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "t5HuHpdKLLiW"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1608359039591,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "D37ElINTLLiW"
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask,combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104921,
     "status": "ok",
     "timestamp": 1608360145815,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "NnPdqj0CLLiW",
    "outputId": "4115468a-d250-426e-9091-d64f827fc13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.4621 Accuracy 0.0858\n",
      "Epoch 1 Batch 50 Loss 6.3468 Accuracy 0.1220\n",
      "Epoch 1 Batch 100 Loss 5.9759 Accuracy 0.1649\n",
      "Epoch 1 Batch 150 Loss 5.7243 Accuracy 0.1928\n",
      "Epoch 1 Batch 200 Loss 5.5363 Accuracy 0.2129\n",
      "Epoch 1 Batch 250 Loss 5.4055 Accuracy 0.2267\n",
      "Epoch 1 Batch 300 Loss 5.2994 Accuracy 0.2381\n",
      "Epoch 1 Batch 350 Loss 5.2102 Accuracy 0.2473\n",
      "Epoch 1 Batch 400 Loss 5.1296 Accuracy 0.2561\n",
      "Epoch 1 Batch 450 Loss 5.0542 Accuracy 0.2648\n",
      "Epoch 1 Batch 500 Loss 4.9816 Accuracy 0.2733\n",
      "Epoch 1 Batch 550 Loss 4.9153 Accuracy 0.2814\n",
      "Epoch 1 Batch 600 Loss 4.8525 Accuracy 0.2892\n",
      "Epoch 1 Batch 650 Loss 4.7942 Accuracy 0.2963\n",
      "Epoch 1 Batch 700 Loss 4.7392 Accuracy 0.3033\n",
      "Epoch 1 Loss 4.7376 Accuracy 0.3034\n",
      "Time taken for 1 epoch: 115.91195440292358 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.8999 Accuracy 0.3890\n",
      "Epoch 2 Batch 50 Loss 3.8268 Accuracy 0.4109\n",
      "Epoch 2 Batch 100 Loss 3.7887 Accuracy 0.4156\n",
      "Epoch 2 Batch 150 Loss 3.7676 Accuracy 0.4182\n",
      "Epoch 2 Batch 200 Loss 3.7395 Accuracy 0.4216\n",
      "Epoch 2 Batch 250 Loss 3.7158 Accuracy 0.4247\n",
      "Epoch 2 Batch 300 Loss 3.6835 Accuracy 0.4287\n",
      "Epoch 2 Batch 350 Loss 3.6571 Accuracy 0.4319\n",
      "Epoch 2 Batch 400 Loss 3.6350 Accuracy 0.4347\n",
      "Epoch 2 Batch 450 Loss 3.6099 Accuracy 0.4381\n",
      "Epoch 2 Batch 500 Loss 3.5902 Accuracy 0.4404\n",
      "Epoch 2 Batch 550 Loss 3.5691 Accuracy 0.4430\n",
      "Epoch 2 Batch 600 Loss 3.5471 Accuracy 0.4457\n",
      "Epoch 2 Batch 650 Loss 3.5243 Accuracy 0.4485\n",
      "Epoch 2 Batch 700 Loss 3.5026 Accuracy 0.4510\n",
      "Epoch 2 Loss 3.5021 Accuracy 0.4510\n",
      "Time taken for 1 epoch: 111.63142800331116 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.9342 Accuracy 0.4946\n",
      "Epoch 3 Batch 50 Loss 2.9738 Accuracy 0.5085\n",
      "Epoch 3 Batch 100 Loss 2.9413 Accuracy 0.5117\n",
      "Epoch 3 Batch 150 Loss 2.9310 Accuracy 0.5137\n",
      "Epoch 3 Batch 200 Loss 2.9068 Accuracy 0.5170\n",
      "Epoch 3 Batch 250 Loss 2.8969 Accuracy 0.5181\n",
      "Epoch 3 Batch 300 Loss 2.8854 Accuracy 0.5191\n",
      "Epoch 3 Batch 350 Loss 2.8735 Accuracy 0.5205\n",
      "Epoch 3 Batch 400 Loss 2.8552 Accuracy 0.5226\n",
      "Epoch 3 Batch 450 Loss 2.8408 Accuracy 0.5241\n",
      "Epoch 3 Batch 500 Loss 2.8253 Accuracy 0.5261\n",
      "Epoch 3 Batch 550 Loss 2.8094 Accuracy 0.5281\n",
      "Epoch 3 Batch 600 Loss 2.7965 Accuracy 0.5293\n",
      "Epoch 3 Batch 650 Loss 2.7801 Accuracy 0.5312\n",
      "Epoch 3 Batch 700 Loss 2.7668 Accuracy 0.5327\n",
      "Epoch 3 Loss 2.7660 Accuracy 0.5328\n",
      "Time taken for 1 epoch: 111.1330292224884 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.2772 Accuracy 0.5854\n",
      "Epoch 4 Batch 50 Loss 2.2633 Accuracy 0.5923\n",
      "Epoch 4 Batch 100 Loss 2.2643 Accuracy 0.5918\n",
      "Epoch 4 Batch 150 Loss 2.2749 Accuracy 0.5901\n",
      "Epoch 4 Batch 200 Loss 2.2719 Accuracy 0.5901\n",
      "Epoch 4 Batch 250 Loss 2.2663 Accuracy 0.5910\n",
      "Epoch 4 Batch 300 Loss 2.2574 Accuracy 0.5923\n",
      "Epoch 4 Batch 350 Loss 2.2500 Accuracy 0.5928\n",
      "Epoch 4 Batch 400 Loss 2.2447 Accuracy 0.5931\n",
      "Epoch 4 Batch 450 Loss 2.2396 Accuracy 0.5939\n",
      "Epoch 4 Batch 500 Loss 2.2344 Accuracy 0.5946\n",
      "Epoch 4 Batch 550 Loss 2.2270 Accuracy 0.5954\n",
      "Epoch 4 Batch 600 Loss 2.2230 Accuracy 0.5957\n",
      "Epoch 4 Batch 650 Loss 2.2151 Accuracy 0.5967\n",
      "Epoch 4 Batch 700 Loss 2.2097 Accuracy 0.5972\n",
      "Epoch 4 Loss 2.2096 Accuracy 0.5973\n",
      "Time taken for 1 epoch: 110.62452483177185 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.9040 Accuracy 0.6514\n",
      "Epoch 5 Batch 50 Loss 1.8412 Accuracy 0.6432\n",
      "Epoch 5 Batch 100 Loss 1.8378 Accuracy 0.6432\n",
      "Epoch 5 Batch 150 Loss 1.8318 Accuracy 0.6443\n",
      "Epoch 5 Batch 200 Loss 1.8333 Accuracy 0.6437\n",
      "Epoch 5 Batch 250 Loss 1.8272 Accuracy 0.6448\n",
      "Epoch 5 Batch 300 Loss 1.8302 Accuracy 0.6440\n",
      "Epoch 5 Batch 350 Loss 1.8259 Accuracy 0.6447\n",
      "Epoch 5 Batch 400 Loss 1.8259 Accuracy 0.6446\n",
      "Epoch 5 Batch 450 Loss 1.8212 Accuracy 0.6448\n",
      "Epoch 5 Batch 500 Loss 1.8201 Accuracy 0.6446\n",
      "Epoch 5 Batch 550 Loss 1.8201 Accuracy 0.6446\n",
      "Epoch 5 Batch 600 Loss 1.8208 Accuracy 0.6444\n",
      "Epoch 5 Batch 650 Loss 1.8213 Accuracy 0.6444\n",
      "Epoch 5 Batch 700 Loss 1.8190 Accuracy 0.6445\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
      "Epoch 5 Loss 1.8190 Accuracy 0.6445\n",
      "Time taken for 1 epoch: 110.94359850883484 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.5152 Accuracy 0.6812\n",
      "Epoch 6 Batch 50 Loss 1.5142 Accuracy 0.6882\n",
      "Epoch 6 Batch 100 Loss 1.5135 Accuracy 0.6882\n",
      "Epoch 6 Batch 150 Loss 1.5137 Accuracy 0.6878\n",
      "Epoch 6 Batch 200 Loss 1.5126 Accuracy 0.6883\n",
      "Epoch 6 Batch 250 Loss 1.5186 Accuracy 0.6866\n",
      "Epoch 6 Batch 300 Loss 1.5219 Accuracy 0.6859\n",
      "Epoch 6 Batch 350 Loss 1.5219 Accuracy 0.6860\n",
      "Epoch 6 Batch 400 Loss 1.5241 Accuracy 0.6854\n",
      "Epoch 6 Batch 450 Loss 1.5268 Accuracy 0.6848\n",
      "Epoch 6 Batch 500 Loss 1.5256 Accuracy 0.6847\n",
      "Epoch 6 Batch 550 Loss 1.5299 Accuracy 0.6838\n",
      "Epoch 6 Batch 600 Loss 1.5326 Accuracy 0.6833\n",
      "Epoch 6 Batch 650 Loss 1.5349 Accuracy 0.6829\n",
      "Epoch 6 Batch 700 Loss 1.5376 Accuracy 0.6824\n",
      "Epoch 6 Loss 1.5375 Accuracy 0.6824\n",
      "Time taken for 1 epoch: 110.5817539691925 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2245 Accuracy 0.7360\n",
      "Epoch 7 Batch 50 Loss 1.2671 Accuracy 0.7257\n",
      "Epoch 7 Batch 100 Loss 1.2635 Accuracy 0.7263\n",
      "Epoch 7 Batch 150 Loss 1.2789 Accuracy 0.7230\n",
      "Epoch 7 Batch 200 Loss 1.2816 Accuracy 0.7223\n",
      "Epoch 7 Batch 250 Loss 1.2884 Accuracy 0.7208\n",
      "Epoch 7 Batch 300 Loss 1.2911 Accuracy 0.7198\n",
      "Epoch 7 Batch 350 Loss 1.2928 Accuracy 0.7191\n",
      "Epoch 7 Batch 400 Loss 1.2959 Accuracy 0.7184\n",
      "Epoch 7 Batch 450 Loss 1.2994 Accuracy 0.7179\n",
      "Epoch 7 Batch 500 Loss 1.3025 Accuracy 0.7173\n",
      "Epoch 7 Batch 550 Loss 1.3047 Accuracy 0.7166\n",
      "Epoch 7 Batch 600 Loss 1.3080 Accuracy 0.7158\n",
      "Epoch 7 Batch 650 Loss 1.3119 Accuracy 0.7152\n",
      "Epoch 7 Batch 700 Loss 1.3155 Accuracy 0.7146\n",
      "Epoch 7 Loss 1.3159 Accuracy 0.7145\n",
      "Time taken for 1 epoch: 109.11022925376892 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.9237 Accuracy 0.7813\n",
      "Epoch 8 Batch 50 Loss 1.0713 Accuracy 0.7568\n",
      "Epoch 8 Batch 100 Loss 1.0825 Accuracy 0.7551\n",
      "Epoch 8 Batch 150 Loss 1.0921 Accuracy 0.7531\n",
      "Epoch 8 Batch 200 Loss 1.0912 Accuracy 0.7527\n",
      "Epoch 8 Batch 250 Loss 1.0986 Accuracy 0.7512\n",
      "Epoch 8 Batch 300 Loss 1.1021 Accuracy 0.7501\n",
      "Epoch 8 Batch 350 Loss 1.1057 Accuracy 0.7492\n",
      "Epoch 8 Batch 400 Loss 1.1061 Accuracy 0.7492\n",
      "Epoch 8 Batch 450 Loss 1.1091 Accuracy 0.7486\n",
      "Epoch 8 Batch 500 Loss 1.1116 Accuracy 0.7480\n",
      "Epoch 8 Batch 550 Loss 1.1148 Accuracy 0.7474\n",
      "Epoch 8 Batch 600 Loss 1.1188 Accuracy 0.7465\n",
      "Epoch 8 Batch 650 Loss 1.1258 Accuracy 0.7452\n",
      "Epoch 8 Batch 700 Loss 1.1303 Accuracy 0.7442\n",
      "Epoch 8 Loss 1.1303 Accuracy 0.7442\n",
      "Time taken for 1 epoch: 107.71658992767334 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.8789 Accuracy 0.7981\n",
      "Epoch 9 Batch 50 Loss 0.9143 Accuracy 0.7833\n",
      "Epoch 9 Batch 100 Loss 0.9185 Accuracy 0.7826\n",
      "Epoch 9 Batch 150 Loss 0.9218 Accuracy 0.7826\n",
      "Epoch 9 Batch 200 Loss 0.9277 Accuracy 0.7814\n",
      "Epoch 9 Batch 250 Loss 0.9299 Accuracy 0.7810\n",
      "Epoch 9 Batch 300 Loss 0.9376 Accuracy 0.7792\n",
      "Epoch 9 Batch 350 Loss 0.9410 Accuracy 0.7783\n",
      "Epoch 9 Batch 400 Loss 0.9436 Accuracy 0.7780\n",
      "Epoch 9 Batch 450 Loss 0.9485 Accuracy 0.7769\n",
      "Epoch 9 Batch 500 Loss 0.9531 Accuracy 0.7759\n",
      "Epoch 9 Batch 550 Loss 0.9579 Accuracy 0.7748\n",
      "Epoch 9 Batch 600 Loss 0.9630 Accuracy 0.7737\n",
      "Epoch 9 Batch 650 Loss 0.9681 Accuracy 0.7725\n",
      "Epoch 9 Batch 700 Loss 0.9734 Accuracy 0.7712\n",
      "Epoch 9 Loss 0.9735 Accuracy 0.7712\n",
      "Time taken for 1 epoch: 107.87647557258606 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.7770 Accuracy 0.8119\n",
      "Epoch 10 Batch 50 Loss 0.7734 Accuracy 0.8121\n",
      "Epoch 10 Batch 100 Loss 0.7784 Accuracy 0.8101\n",
      "Epoch 10 Batch 150 Loss 0.7820 Accuracy 0.8086\n",
      "Epoch 10 Batch 200 Loss 0.7889 Accuracy 0.8069\n",
      "Epoch 10 Batch 250 Loss 0.7968 Accuracy 0.8051\n",
      "Epoch 10 Batch 300 Loss 0.8002 Accuracy 0.8043\n",
      "Epoch 10 Batch 350 Loss 0.8042 Accuracy 0.8035\n",
      "Epoch 10 Batch 400 Loss 0.8086 Accuracy 0.8026\n",
      "Epoch 10 Batch 450 Loss 0.8136 Accuracy 0.8013\n",
      "Epoch 10 Batch 500 Loss 0.8175 Accuracy 0.8003\n",
      "Epoch 10 Batch 550 Loss 0.8246 Accuracy 0.7987\n",
      "Epoch 10 Batch 600 Loss 0.8297 Accuracy 0.7976\n",
      "Epoch 10 Batch 650 Loss 0.8342 Accuracy 0.7966\n",
      "Epoch 10 Batch 700 Loss 0.8404 Accuracy 0.7953\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
      "Epoch 10 Loss 0.8404 Accuracy 0.7952\n",
      "Time taken for 1 epoch: 108.66746544837952 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,train_loss.result(),train_accuracy.result()))\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1608360154148,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "Ee5rS_4ALLiW"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, output,False,enc_padding_mask,combined_mask,dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1608360156522,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "i_URNMNMibo5"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result if i < tokenizer_en.vocab_size], fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1608360159081,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "3j6C1C0TieHZ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1404,
     "status": "ok",
     "timestamp": 1608360162028,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "3JrsBqzYigQ0",
    "outputId": "8a8b70c7-509f-4d48-fb23-d92e8bf62c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: this is a problem that we have to deal with .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1608360168305,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "2iddAog3ikC7",
    "outputId": "e5309913-bf36-4b1f-c67f-54f4a069a903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors heard about this idea.\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1927,
     "status": "ok",
     "timestamp": 1608360179343,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "KZhKRMrfioYS",
    "outputId": "a298ff91-1ddb-4213-8968-7deca9dea403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Predicted translation: so i 'm going to share with you a few stories of some magic things that have happened .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2377714,
     "status": "ok",
     "timestamp": 1608358909041,
     "user": {
      "displayName": "Ka Tung Cheng",
      "photoUrl": "",
      "userId": "01448164510831113819"
     },
     "user_tz": -480
    },
    "id": "kQMfdHPfL1zQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Transformer from scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
